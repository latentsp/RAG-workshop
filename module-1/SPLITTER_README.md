# Enhanced Text Splitter Guide

This guide explains the enhanced `do_chunk_text()` function that now supports multiple text splitter types from LangChain, including semantic splitters and other specialized options.

## üöÄ Quick Start

The function maintains backward compatibility, so existing code will continue to work:

```python
from utility import do_chunk_text

# Basic usage (unchanged)
chunks = do_chunk_text(text, chunk_size=500, chunk_overlap=50)

# New: Specify splitter type
chunks = do_chunk_text(text, chunk_size=500, chunk_overlap=50, splitter_type="semantic")
```

## üìã Available Splitter Types

### 1. `recursive` (Default) ‚≠ê
**Best for**: General text documents, articles, books
```python
chunks = do_chunk_text(text, splitter_type="recursive")
```
- Tries to split on paragraphs, then sentences, then words
- Maintains semantic coherence
- **Recommended for most use cases**

### 2. `character`
**Best for**: Simple, predictable splitting
```python
chunks = do_chunk_text(text, splitter_type="character", separator="\n\n")
```
- Splits on a single character or string
- Less intelligent than recursive

### 3. `semantic` (Experimental) üß†
**Best for**: Documents where meaning boundaries are critical
```python
chunks = do_chunk_text(text, splitter_type="semantic")
```
- Uses AI embeddings to find semantic boundaries
- Groups semantically similar sentences together
- Requires OpenAI API key

### 4. `token`
**Best for**: Token-aware applications
```python
chunks = do_chunk_text(text, splitter_type="token", chunk_size=100)
```
- Splits based on token count (not character count)
- Useful for LLM context window management

### 5. `html_header`
**Best for**: HTML documents with clear header structure
```python
chunks = do_chunk_text(html_text, splitter_type="html_header")
```
- Splits HTML by header tags (h1, h2, h3, etc.)
- Preserves document structure

### 6. `html_section`
**Best for**: HTML documents with section-based layout
```python
chunks = do_chunk_text(html_text, splitter_type="html_section")
```
- Similar to html_header but focuses on sections
- Uses XSLT transformations

### 7. `markdown`
**Best for**: Markdown documents
```python
chunks = do_chunk_text(markdown_text, splitter_type="markdown")
```
- Splits on Markdown headers (#, ##, ###)
- Maintains document hierarchy

### 8. `python`
**Best for**: Python source code
```python
chunks = do_chunk_text(python_code, splitter_type="python")
```
- Understands Python syntax
- Splits on functions, classes, etc.

### 9. `latex`
**Best for**: LaTeX documents
```python
chunks = do_chunk_text(latex_text, splitter_type="latex")
```
- Understands LaTeX structure
- Splits on sections, subsections, etc.

### 10. `nltk`
**Best for**: Natural language processing
```python
chunks = do_chunk_text(text, splitter_type="nltk")
```
- Uses NLTK for sentence boundary detection
- More accurate sentence splitting

### 11. `spacy`
**Best for**: Advanced NLP applications
```python
chunks = do_chunk_text(text, splitter_type="spacy", pipeline="en_core_web_sm")
```
- Uses spaCy for intelligent text processing
- Supports multiple languages

## üîß Advanced Configuration

### Custom Separators
```python
chunks = do_chunk_text(
    text,
    splitter_type="recursive",
    separators=["\n\n", ".", "!", "?", " "]
)
```

### HTML Header Configuration
```python
chunks = do_chunk_text(
    html_text,
    splitter_type="html_header",
    headers_to_split_on=[
        ("h1", "Header 1"),
        ("h2", "Header 2"),
        ("h3", "Header 3"),
    ]
)
```

### Semantic Splitter Options
```python
chunks = do_chunk_text(
    text,
    splitter_type="semantic",
    breakpoint_threshold_type="percentile"  # or "standard_deviation"
)
```

## üõ†Ô∏è Setup Instructions

### 1. Create Virtual Environment
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 2. Install Dependencies
```bash
pip install python-dotenv
pip install -r requirements.txt
```

### 3. Set Environment Variables
Create a `.env` file in the module-1 directory:
```env
OPENAI_API_KEY=your_openai_api_key_here
```

### 4. Test Installation
```bash
python3 test_splitters.py
```

## üìä Performance Comparison

Run the example script to see how different splitters perform:

```bash
python3 splitter_examples.py
```

This will show:
- Number of chunks generated by each splitter
- Average chunk sizes
- Preview of chunk content

## üéØ Choosing the Right Splitter

| Use Case | Recommended Splitter | Why |
|----------|---------------------|-----|
| Books, articles, general text | `recursive` | Maintains readability and context |
| Research papers | `semantic` | Preserves semantic meaning |
| HTML documentation | `html_header` | Respects document structure |
| Code documentation | `python`, `markdown` | Syntax-aware splitting |
| Token-sensitive applications | `token` | Precise token control |
| Multilingual content | `spacy` | Advanced language support |

## üîç Example Usage

### Basic Text Splitting
```python
from utility import do_load_document, do_chunk_text

# Load document
text = do_load_document("alice_in_wonderland_book.txt")

# Split with different methods
recursive_chunks = do_chunk_text(text, chunk_size=500, splitter_type="recursive")
semantic_chunks = do_chunk_text(text, splitter_type="semantic")
token_chunks = do_chunk_text(text, chunk_size=100, splitter_type="token")

print(f"Recursive: {len(recursive_chunks)} chunks")
print(f"Semantic: {len(semantic_chunks)} chunks")
print(f"Token: {len(token_chunks)} chunks")
```

### HTML Document Processing
```python
html_content = """
<h1>Chapter 1</h1>
<p>Introduction text here...</p>
<h2>Section 1</h2>
<p>Section content...</p>
"""

html_chunks = do_chunk_text(html_content, splitter_type="html_header")
```

### Code Splitting
```python
python_code = '''
def hello_world():
    print("Hello, World!")

class MyClass:
    def __init__(self):
        pass
'''

code_chunks = do_chunk_text(python_code, splitter_type="python")
```

## üö® Error Handling

The function includes robust error handling:

1. **Missing Dependencies**: Automatically attempts installation or provides instructions
2. **Invalid Splitter Type**: Falls back to recursive splitter
3. **API Errors**: Graceful degradation for semantic splitter

## üîÑ Backward Compatibility

All existing code using `do_chunk_text()` will continue to work unchanged. The function defaults to the recursive splitter, which was the original behavior.

## üìù Notes

- **Semantic Splitter**: Requires OpenAI API key and `langchain-experimental`
- **NLTK/SpaCy**: May require additional model downloads
- **HTML Splitters**: Work best with well-formed HTML
- **Token Splitter**: Uses different encoding models (default: GPT-2)

## üéâ Benefits

1. **Flexibility**: Choose the right splitter for your content type
2. **Quality**: Better chunk boundaries with semantic and syntax-aware splitters
3. **Compatibility**: Existing code continues to work
4. **Extensibility**: Easy to add new splitter types

## üêõ Troubleshooting

### Common Issues

1. **Module not found**: Run dependency installation
2. **API key errors**: Check `.env` file setup
3. **Memory issues with semantic splitter**: Use smaller text chunks
4. **HTML parsing errors**: Ensure valid HTML input

### Getting Help

- Check `test_splitters.py` for dependency status
- Run `splitter_examples.py` for working examples
- Review error messages for specific guidance

---

Happy chunking! üéØ 