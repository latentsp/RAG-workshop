{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9235c7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # This will load variables from a .env file into the environment\n",
    "\n",
    "# Get your OpenAI API key from the environment\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment. Please set it in your .env file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e9b948d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: Hello! I'm just a language model, so I don't have feelings, but I'm here to help you with any questions or conversations you'd like to have. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "def do_invoke_llm(prompt):\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain_core.messages import HumanMessage\n",
    "\n",
    "    llm = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    print(\"LLM Response:\", response.content)\n",
    "\n",
    "do_invoke_llm(\"Hello, LLM! How are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e90f2c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# RAG (Retrieval-Augmented Generation) Workshop\n",
    "\n",
    "This notebook demonstrates the two main steps of RAG:\n",
    "\n",
    "1. **Offline - Document Storage** ğŸ—„ï¸: Loading, chunking, embedding, and storing documents in a vector database (FAISS)\n",
    "2. **Online - Information Retrieval** ğŸ”: Retrieving relevant chunks based on user queries and generating responses\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "RAG combines the power of information retrieval with large language models. Instead of relying solely on the model's training data, RAG allows us to:\n",
    "- Provide up-to-date information\n",
    "- Access domain-specific knowledge\n",
    "- Reduce hallucinations\n",
    "- Cite sources\n",
    "\n",
    "âœ¨ Let's walk through each step of the process! ğŸ“\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f318c395",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 1: Offline - Document Storage\n",
    "\n",
    "### 1.1 Document Loading\n",
    "First, we need to load our document. We'll use a simple text file about AI and Machine Learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441c3d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_load_document(file_path):\n",
    "    \"\"\"Load a text document from the specified file path.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        print(f\"âœ… Document loaded successfully!\")\n",
    "        print(f\"ğŸ“„ Document length: {len(content)} characters\")\n",
    "        print(f\"ğŸ”¤ First 200 characters:\\n{content[:200]}...\")\n",
    "        return content\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Error: File '{file_path}' not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading document: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11ace10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Document loaded successfully!\n",
      "ğŸ“„ Document length: 163912 characters\n",
      "ğŸ”¤ First 200 characters:\n",
      "The Project Gutenberg eBook of Alice's Adventures in Wonderland\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no r...\n"
     ]
    }
   ],
   "source": [
    "# Load our sample document\n",
    "document_content = do_load_document(\"sample_document.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbb1368",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 1.2 Document Chunking\n",
    "Large documents need to be split into smaller chunks for efficient retrieval and to fit within LLM context windows. We'll use LangChain's text splitter to create overlapping chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f04692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_chunk_text(text, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"Split text into overlapping chunks for better retrieval.\"\"\"\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    \n",
    "    # Create text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    # Split the text\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    print(f\"âœ… Text chunked successfully!\")\n",
    "    print(f\"ğŸ“Š Number of chunks: {len(chunks)}\")\n",
    "    print(f\"ğŸ“ Average chunk size: {sum(len(chunk) for chunk in chunks) / len(chunks):.0f} characters\")\n",
    "    print(f\"\\nğŸ” First chunk preview:\\n{chunks[0][:300]}...\")\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a412756d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Text chunked successfully!\n",
      "ğŸ“Š Number of chunks: 452\n",
      "ğŸ“ Average chunk size: 364 characters\n",
      "\n",
      "ğŸ” First chunk preview:\n",
      "The Project Gutenberg eBook of Alice's Adventures in Wonderland\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gu...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Only chunk if we successfully loaded the document\n",
    "if document_content:\n",
    "    text_chunks = do_chunk_text(document_content)\n",
    "else:\n",
    "    print(\"âŒ Cannot chunk text - document not loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ba6ce2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 1.3 Creating Embeddings and Vector Store\n",
    "Now we'll convert our text chunks into vector embeddings and store them in a FAISS vector database for efficient similarity search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "114674e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_create_vector_store(chunks):\n",
    "    \"\"\"Create embeddings and FAISS vector store from text chunks.\"\"\"\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    from langchain_community.vectorstores import FAISS\n",
    "    \n",
    "    # Create embeddings model\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "    \n",
    "    print(\"ğŸ”„ Creating embeddings and vector store...\")\n",
    "    print(\"â³ This may take a moment...\")\n",
    "    \n",
    "    # Create FAISS vector store\n",
    "    from langchain_community.vectorstores import Chroma\n",
    "    vector_store = Chroma.from_texts(chunks, embeddings)\n",
    "    \n",
    "    print(f\"âœ… Vector store created successfully!\")\n",
    "    print(f\"ğŸ—ƒï¸ Stored {len(chunks)} document chunks\")\n",
    "    print(f\"ğŸ§® Each embedding has {len(vector_store.embeddings.embed_query('test'))} dimensions\")\n",
    "    \n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1beb7eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Creating embeddings and vector store...\n",
      "â³ This may take a moment...\n",
      "âœ… Vector store created successfully!\n",
      "ğŸ—ƒï¸ Stored 452 document chunks\n",
      "ğŸ§® Each embedding has 1536 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Create vector store if we have chunks\n",
    "if 'text_chunks' in locals() and text_chunks:\n",
    "    vector_store = do_create_vector_store(text_chunks)\n",
    "else:\n",
    "    print(\"âŒ Cannot create vector store - no text chunks available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192c1f35",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 2: Information Retrieval\n",
    "\n",
    "### 2.1 Similarity Search\n",
    "Now we can search for relevant chunks based on user queries using vector similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9e08e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Searching for: 'Did they ask alice to drink them?'\n",
      "ğŸ“Š Retrieving top 3 most relevant chunks...\n",
      "\n",
      "âœ… Found 3 relevant chunks:\n",
      "================================================================================\n",
      "\n",
      "ğŸ”¸ Chunk 1:\n",
      "ğŸ“ Content: It was all very well to say â€œDrink me,â€ but the wise little Alice was\n",
      "not going to do _that_ in a hurry. â€œNo, Iâ€™ll look first,â€ she said,\n",
      "â€œand see whether itâ€™s marked â€˜_poison_â€™ or notâ€; for she had read\n",
      "several nice little histories about children who had got burnt, and\n",
      "eaten up by wild beasts and ...\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ”¸ Chunk 2:\n",
      "ğŸ“ Content: There seemed to be no use in waiting by the little door, so she went\n",
      "back to the table, half hoping she might find another key on it, or at\n",
      "any rate a book of rules for shutting people up like telescopes: this\n",
      "time she found a little bottle on it, (â€œwhich certainly was not here\n",
      "before,â€ said Alice,)...\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ”¸ Chunk 3:\n",
      "ğŸ“ Content: The great question certainly was, what? Alice looked all round her at\n",
      "the flowers and the blades of grass, but she did not see anything that\n",
      "looked like the right thing to eat or drink under the circumstances.\n",
      "There was a large mushroom growing near her, about the same height as\n",
      "herself; and when sh...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def do_similarity_search(vector_store, query, k=3):\n",
    "    \"\"\"Search for the most relevant chunks based on the query.\"\"\"\n",
    "    print(f\"ğŸ” Searching for: '{query}'\")\n",
    "    print(f\"ğŸ“Š Retrieving top {k} most relevant chunks...\\n\")\n",
    "    \n",
    "    # Perform similarity search\n",
    "    relevant_docs = vector_store.similarity_search(query, k=k)\n",
    "    \n",
    "    print(f\"âœ… Found {len(relevant_docs)} relevant chunks:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, doc in enumerate(relevant_docs, 1):\n",
    "        print(f\"\\nğŸ”¸ Chunk {i}:\")\n",
    "        print(f\"ğŸ“ Content: {doc.page_content[:300]}...\")\n",
    "        print(\"-\"*60)\n",
    "    \n",
    "    return relevant_docs\n",
    "\n",
    "# Test similarity search with a sample query\n",
    "if 'vector_store' in locals():\n",
    "    query = \"Did they ask alice to drink them?\"\n",
    "    relevant_chunks = do_similarity_search(vector_store, query)\n",
    "else:\n",
    "    print(\"âŒ Cannot perform search - vector store not available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a48887",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 2.2 RAG Pipeline - Putting It All Together\n",
    "Now we'll combine retrieval with generation to create a complete RAG system that can answer questions based on our document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18659aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Testing complete RAG pipeline:\n",
      "\n",
      "\n",
      "==================== Test Query 1 ====================\n",
      "ğŸ¯ RAG Query: 'Did they ask alice to drink them?'\n",
      "================================================================================\n",
      "ğŸ” Step 1: Retrieving relevant information...\n",
      "âœ… Retrieved 3 relevant chunks\n",
      "ğŸ“„ Total context length: 1350 characters\n",
      "\n",
      "ğŸ¤– Step 2: Generating response...\n",
      "âœ… Response generated!\n",
      "================================================================================\n",
      "ğŸ”Š RAG Response:\n",
      "Yes, the bottle had a paper label with the words \"DRINK ME\" printed on it in large letters.\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def do_rag_query(vector_store, query, k=3):\n",
    "    \"\"\"Complete RAG pipeline: retrieve relevant chunks and generate response.\"\"\"\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain_core.messages import HumanMessage\n",
    "    \n",
    "    print(f\"ğŸ¯ RAG Query: '{query}'\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Retrieve relevant chunks\n",
    "    print(\"ğŸ” Step 1: Retrieving relevant information...\")\n",
    "    relevant_docs = vector_store.similarity_search(query, k=k)\n",
    "    \n",
    "    # Combine retrieved content\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    \n",
    "    print(f\"âœ… Retrieved {len(relevant_docs)} relevant chunks\")\n",
    "    print(f\"ğŸ“„ Total context length: {len(context)} characters\")\n",
    "    \n",
    "    # Step 2: Generate response using LLM\n",
    "    print(\"\\nğŸ¤– Step 2: Generating response...\")\n",
    "    \n",
    "    # Create prompt with context\n",
    "    prompt = f\"\"\"Based on the following context, please answer the question. If the answer is not in the context, say so.\n",
    "\n",
    "                Context:\n",
    "                {context}\n",
    "\n",
    "                Question: \n",
    "                {query}\n",
    "\n",
    "                Answer:\"\"\"\n",
    "    \n",
    "    # Generate response\n",
    "    llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=0)\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    print(\"âœ… Response generated!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"ğŸ”Š RAG Response:\")\n",
    "    print(response.content)\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        'query': query,\n",
    "        'retrieved_chunks': relevant_docs,\n",
    "        'context': context,\n",
    "        'response': response.content\n",
    "    }\n",
    "\n",
    "# Test the complete RAG pipeline\n",
    "if 'vector_store' in locals():\n",
    "    print(\"ğŸš€ Testing complete RAG pipeline:\\n\")\n",
    "    \n",
    "    # Test multiple queries\n",
    "    test_queries = [\n",
    "        \"Did they ask alice to drink them?\"\n",
    "    ]\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\n{'='*20} Test Query {i} {'='*20}\")\n",
    "        result = do_rag_query(vector_store, query)\n",
    "        print(\"\\n\")\n",
    "else:\n",
    "    print(\"âŒ Cannot run RAG pipeline - vector store not available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc144494",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully implemented a complete RAG system. Here's what we accomplished:\n",
    "\n",
    "### âœ… What We Built:\n",
    "1. **Document Loading**: Read a text file about AI/ML\n",
    "2. **Text Chunking**: Split the document into manageable pieces with overlap\n",
    "3. **Embeddings**: Converted text chunks into vector representations\n",
    "4. **Vector Store**: Stored embeddings in FAISS for efficient similarity search\n",
    "5. **Retrieval**: Found relevant chunks based on user queries\n",
    "6. **Generation**: Combined retrieved context with LLM to generate informed responses\n",
    "\n",
    "### ğŸ¯ Key Benefits of RAG:\n",
    "- **Current Information**: Can work with up-to-date documents\n",
    "- **Domain Expertise**: Provides specialized knowledge beyond training data\n",
    "- **Reduced Hallucination**: Grounds responses in actual source material\n",
    "- **Traceable Sources**: Can identify which chunks informed the response\n",
    "\n",
    "### ğŸ”„ Try Your Own Query:\n",
    "Modify the test query below to ask your own questions about the AI/ML document!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34d9702b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Testing your custom query...\n",
      "ğŸ¯ RAG Query: 'What is the difference between supervised and unsupervised learning?'\n",
      "================================================================================\n",
      "ğŸ” Step 1: Retrieving relevant information...\n",
      "âœ… Retrieved 3 relevant chunks\n",
      "ğŸ“„ Total context length: 1320 characters\n",
      "\n",
      "ğŸ¤– Step 2: Generating response...\n",
      "âœ… Response generated!\n",
      "================================================================================\n",
      "ğŸ”Š RAG Response:\n",
      "The answer is not in the context provided.\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ’¡ Pro Tips:\n",
      "- Try questions about specific AI/ML concepts from the document\n",
      "- Ask about applications, challenges, or comparisons\n",
      "- Notice how the system retrieves relevant chunks first\n",
      "- Observe how the LLM uses the context to generate responses\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ® Interactive RAG Testing\n",
    "# Change the query below to test your own questions!\n",
    "\n",
    "if 'vector_store' in locals():\n",
    "    # Try your own query here!\n",
    "    custom_query = \"What is the difference between supervised and unsupervised learning?\"\n",
    "    \n",
    "    print(\"ğŸ¯ Testing your custom query...\")\n",
    "    result = do_rag_query(vector_store, custom_query)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ’¡ Pro Tips:\")\n",
    "    print(\"- Try questions about specific AI/ML concepts from the document\")\n",
    "    print(\"- Ask about applications, challenges, or comparisons\")\n",
    "    print(\"- Notice how the system retrieves relevant chunks first\")\n",
    "    print(\"- Observe how the LLM uses the context to generate responses\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"âŒ Vector store not available. Please run all previous cells first.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
